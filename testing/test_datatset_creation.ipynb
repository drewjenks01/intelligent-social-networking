{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datatset of faces\n",
    "# if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "#     !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('ibug_300W_large_face_landmark_dataset/lfpw/testset')\n",
    "\n",
    "train_image_paths = []\n",
    "test_image_paths = []\n",
    "\n",
    "i=0\n",
    "while len(train_image_paths) < 150:\n",
    "    i+=1\n",
    "    # i into 3 digit number\n",
    "    if i < 10:\n",
    "        filename = f'image_000{i}'\n",
    "    elif i < 100:\n",
    "        filename = f'image_00{i}'\n",
    "    else:\n",
    "        filename = f'image_0{i}'\n",
    "\n",
    "    if filename+'.png' not in image_paths:\n",
    "        continue\n",
    "    \n",
    "    train_image_paths.append(os.path.join('ibug_300W_large_face_landmark_dataset/lfpw/testset', filename+'.png'))\n",
    "    test_image_paths.append(os.path.join('ibug_300W_large_face_landmark_dataset/lfpw/testset', filename+'_mirror.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths), len(test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0001.png',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0002.png',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0003.png',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0004.png',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0005.png'],\n",
       " ['ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0001_mirror.jpg',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0002_mirror.jpg',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0003_mirror.jpg',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0004_mirror.jpg',\n",
       "  'ibug_300W_large_face_landmark_dataset/lfpw/testset/image_0005_mirror.jpg'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[:5], test_image_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewjenkins/anaconda3/envs/intelligent_sn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facial_detection.facial_detection import FacialDetection\n",
    "\n",
    "facial_detection = FacialDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [01:06,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# facial embeddings -> image file name\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "train_data = {}\n",
    "ground_truth = {}\n",
    "train_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "for train_path, test_path in tqdm(zip(train_image_paths, test_image_paths)):\n",
    "    train_image = Image.open(train_path).convert('RGB')\n",
    "    test_image = Image.open(test_path).convert('RGB')\n",
    "\n",
    "    detected_face = facial_detection.detect_face(train_image)\n",
    "\n",
    "    embedding = facial_detection.get_facial_embeddings(detected_face)\n",
    "\n",
    "    train_embeddings.append(embedding)\n",
    "    train_data[embedding] = train_path.split('/')[-1]\n",
    "\n",
    "    detected_face = facial_detection.detect_face(test_image)\n",
    "    embedding = facial_detection.get_facial_embeddings(detected_face)\n",
    "    test_embeddings.append(embedding)\n",
    "    ground_truth[embedding] = train_path.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('image_0001.png', 'image_0001.png')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[test_embeddings[0]], train_data[train_embeddings[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nmslib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatabase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Database\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# method -> (speed, accuracy)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/classes/intelligent-social-networking/database/db.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlshashpy3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlshash\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnmslib\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDatabase\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nmslib'"
     ]
    }
   ],
   "source": [
    "from database.db import Database\n",
    "import time\n",
    "\n",
    "# method -> (speed, accuracy)\n",
    "collected_metrics = {}\n",
    "\n",
    "# collect speed and accuracy for each search method\n",
    "for search_method in ['lsh','hnsw','vector_compression','linear']:\n",
    "    db = Database(search_method)\n",
    "\n",
    "    for embedding in train_embeddings:\n",
    "        db.add_entry(embedding, train_data[embedding])\n",
    "\n",
    "    correct = 0\n",
    "    start_time = time.time()\n",
    "    for embedding in test_embeddings:\n",
    "        result = db.query(embedding)\n",
    "\n",
    "        if result == ground_truth[embedding]:\n",
    "            correct+=1\n",
    "\n",
    "    end_time = time.time()\n",
    "    collected_metrics[search_method] = (len(test_embeddings)/(end_time-start_time), correct/len(test_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plots for each metric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "speeds = [collected_metrics[method][0] for method in collected_metrics]\n",
    "accuracies = [collected_metrics[method][1] for method in collected_metrics]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].bar(collected_metrics.keys(), speeds)\n",
    "ax[0].set_title('Speeds')\n",
    "ax[0].set_ylabel('Images per second')\n",
    "ax[1].bar(collected_metrics.keys(), accuracies)\n",
    "ax[1].set_title('Accuracies')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent_sn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
